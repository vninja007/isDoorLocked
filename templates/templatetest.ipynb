{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correlation match score: 0.998\n",
      "xmatch = 446 ymatch = 194\n",
      "correlation match score: 0.986\n",
      "xmatch = 434 ymatch = 160\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# read image\n",
    "imgopen = cv2.imread('../doho.png')\n",
    "imgclosed = cv2.imread('../dchc.png')\n",
    "tmplt_handle = cv2.imread('template_open2.png', cv2.IMREAD_UNCHANGED)\n",
    "tmplt_deadbolt = cv2.imread('template_deadbolt.png', cv2.IMREAD_UNCHANGED)\n",
    "\n",
    "\n",
    "def getMatch(img, tmplt):\n",
    "\n",
    "# read template with alpha\n",
    "    hh, ww = tmplt.shape[:2]\n",
    "    tmplt_mask = tmplt[:,:,3]\n",
    "    tmplt_mask = cv2.merge([tmplt_mask,tmplt_mask,tmplt_mask])\n",
    "    tmplt2 = tmplt[:,:,0:3]\n",
    "    corrimg = cv2.matchTemplate(img,tmplt2,cv2.TM_CCORR_NORMED, mask=tmplt_mask)\n",
    "    min_val, max_val, min_loc, max_loc = cv2.minMaxLoc(corrimg)\n",
    "    max_val_ncc = '{:.3f}'.format(max_val)\n",
    "    print(\"correlation match score: \" + max_val_ncc)\n",
    "    xx = max_loc[0]\n",
    "    yy = max_loc[1]\n",
    "    print('xmatch =',xx,'ymatch =',yy)\n",
    "\n",
    "    # draw red bounding box to define match location\n",
    "    result = img.copy()\n",
    "    pt1 = (xx,yy)\n",
    "    pt2 = (xx+ww, yy+hh)\n",
    "    cv2.rectangle(result, pt1, pt2, (0,0,255), 1)\n",
    "    newimg = img[pt1[1]:pt2[1], pt1[0]:pt2[0]]\n",
    "    newimg = newimg[len(newimg)//3:2*len(newimg)//3, len(newimg[0])//3:2*len(newimg[0])//3]\n",
    "    # cv2.imshow('result', result)\n",
    "    # cv2.waitKey(0)\n",
    "    # cv2.destroyAllWindows()\n",
    "    return newimg\n",
    "\n",
    "# save results\n",
    "ohandle = getMatch(imgopen, tmplt_deadbolt)\n",
    "chandle = getMatch(imgclosed, tmplt_deadbolt)\n",
    "cv2.imwrite('cd.png',chandle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity Score: 79.355%\n"
     ]
    }
   ],
   "source": [
    "from skimage.metrics import structural_similarity\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "first = ohandle\n",
    "second = chandle\n",
    "\n",
    "# Convert images to grayscale\n",
    "first_gray = cv2.cvtColor(first, cv2.COLOR_BGR2GRAY)\n",
    "second_gray = cv2.cvtColor(second, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Compute SSIM between two images\n",
    "score, diff = structural_similarity(first_gray, second_gray, full=True)\n",
    "print(\"Similarity Score: {:.3f}%\".format(score * 100))\n",
    "\n",
    "# The diff image contains the actual image differences between the two images\n",
    "# and is represented as a floating point data type so we must convert the array \n",
    "# to 8-bit unsigned integers in the range [0,255] before we can use it with OpenCV\n",
    "diff = (diff * 255).astype(\"uint8\")\n",
    "\n",
    "# Threshold the difference image, followed by finding contours to\n",
    "# obtain the regions that differ between the two images\n",
    "thresh = cv2.threshold(diff, 0, 255, cv2.THRESH_BINARY_INV | cv2.THRESH_OTSU)[1]\n",
    "contours = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "contours = contours[0] if len(contours) == 2 else contours[1]\n",
    "\n",
    "# Highlight differences\n",
    "mask = np.zeros(first.shape, dtype='uint8')\n",
    "filled = second.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "from PIL import Image\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Load the OpenAI CLIP Model\n",
    "print('Loading CLIP Model...')\n",
    "model = SentenceTransformer('clip-ViT-B-32')\n",
    "\n",
    "# Next we compute the embeddings\n",
    "# To encode an image, you can use the following code:\n",
    "# from PIL import Image\n",
    "# encoded_image = model.encode(Image.open(filepath))\n",
    "image_names = list(glob.glob('./*.jpg'))\n",
    "print(\"Images:\", len(image_names))\n",
    "encoded_image = model.encode([Image.open(filepath) for filepath in image_names], batch_size=128, convert_to_tensor=True, show_progress_bar=True)\n",
    "\n",
    "# Now we run the clustering algorithm. This function compares images aganist \n",
    "# all other images and returns a list with the pairs that have the highest \n",
    "# cosine similarity score\n",
    "processed_images = util.paraphrase_mining_embeddings(encoded_image)\n",
    "NUM_SIMILAR_IMAGES = 10 \n",
    "\n",
    "# =================\n",
    "# DUPLICATES\n",
    "# =================\n",
    "print('Finding duplicate images...')\n",
    "# Filter list for duplicates. Results are triplets (score, image_id1, image_id2) and is scorted in decreasing order\n",
    "# A duplicate image will have a score of 1.00\n",
    "# It may be 0.9999 due to lossy image compression (.jpg)\n",
    "duplicates = [image for image in processed_images if image[0] >= 0.999]\n",
    "\n",
    "# Output the top X duplicate images\n",
    "for score, image_id1, image_id2 in duplicates[0:NUM_SIMILAR_IMAGES]:\n",
    "    print(\"\\nScore: {:.3f}%\".format(score * 100))\n",
    "    print(image_names[image_id1])\n",
    "    print(image_names[image_id2])\n",
    "\n",
    "# =================\n",
    "# NEAR DUPLICATES\n",
    "# =================\n",
    "print('Finding near duplicate images...')\n",
    "# Use a threshold parameter to identify two images as similar. By setting the threshold lower, \n",
    "# you will get larger clusters which have less similar images in it. Threshold 0 - 1.00\n",
    "# A threshold of 1.00 means the two images are exactly the same. Since we are finding near \n",
    "# duplicate images, we can set it at 0.99 or any number 0 < X < 1.00.\n",
    "threshold = 0.99\n",
    "near_duplicates = [image for image in processed_images if image[0] < threshold]\n",
    "\n",
    "for score, image_id1, image_id2 in near_duplicates[0:NUM_SIMILAR_IMAGES]:\n",
    "    print(\"\\nScore: {:.3f}%\".format(score * 100))\n",
    "    print(image_names[image_id1])\n",
    "    print(image_names[image_id2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "work",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
